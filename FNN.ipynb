{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FNN:\n",
    "#     def __init__(self, input_size=6, hidden_size=[8,8], output_size=1, \n",
    "#                  learning_rate=0.01, activation=\"RELU\"):\n",
    "        \n",
    "#         self.lr=learning_rate\n",
    "        \n",
    "#         self.weights={}\n",
    "#         self.weights['W1']=np.random.randn(input_size, hidden_size[0])\n",
    "#         self.weights['W2']=np.random.randn(hidden_size[0], hidden_size[1])\n",
    "#         self.weights['W3']=np.random.randn(hidden_size[1], output_size)\n",
    "        \n",
    "#         self.bias={}\n",
    "#         self.bias['b1']=np.zeros(shape=(hidden_size[0],))\n",
    "#         self.bias['b2']=np.zeros(shape=(hidden_size[1],))\n",
    "#         self.bias['b3']=np.zeros(shape=(output_size))\n",
    "        \n",
    "#         self.activations=RELU()\n",
    "        \n",
    "#         if activation==\"RELU\":\n",
    "#             self.activatons=RELU()\n",
    "#         elif activation==\"tanh\":\n",
    "#             self.activation=np.tanh()\n",
    "        \n",
    "#     def forward(self, input):\n",
    "#         h1=np.matmul(input, self.weights['W1'])+ self.bias['b1']\n",
    "#         h2=np.matmul(h1,self.weights['W2'])+ self.bias['b2']\n",
    "#         h3=np.matmul(h2, self.weigths['W3']) + self.bias['b3']\n",
    "        \n",
    "#         return h3\n",
    "    \n",
    "# from tds    \n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
    "        # A dummy layer does nothing\n",
    "        self.weights = np.zeros(shape=(input.shape[1], 10))\n",
    "        bias = np.zeros(shape=(10,))\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], returns output data [batch, 10]\n",
    "        \"\"\"\n",
    "        output = np.matmul(input, self.weights) + bias\n",
    "        return output\n",
    "    \n",
    "class RELU(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return np.maximum(0,z)\n",
    "    \n",
    "    def backward(self, z, out_grad):\n",
    "        relu_grad = z > 0\n",
    "        return out_grad*relu_grad   \n",
    "#         relu_grad=[]\n",
    "#         for i in range(len(z)):\n",
    "#             if z[i]>=0:\n",
    "#                 relu_grad[i]=1\n",
    "#             else:\n",
    "#                 relu_grad[i]=0\n",
    "#         return np.dot(np.transpose(out_grad),relu_grad)\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def backward(self, z, out_grad):\n",
    "        return np.dot(np.transpose(out_grad),np.sinh(z))\n",
    "    \n",
    "class Hidden(Layer):\n",
    "    def __init__(self, last_size, hidden_size, learning_rate=0.01,\n",
    "                weight_init_std=0.01):\n",
    "        self.W=weight_init_std * np.random.randn(last_size, hidden_size)\n",
    "        self.b=np.zeros(shape=(hidden_size,))\n",
    "        \n",
    "        self.lr=learning_rate\n",
    "        \n",
    "    def forward(self, X): # forward X = result from last layer(or input)\n",
    "        layer_out= np.dot(X, self.W) + self.b\n",
    "        return layer_out\n",
    "    \n",
    "    def backward(self, x, grad_out): # x=input of data\n",
    "        dW = np.dot(np.transpose(x), grad_out)\n",
    "        db = np.sum(grad_out, axis=0)\n",
    "#        dx = np.dot(np.transpose(grad_out), self.W)\n",
    "        dx = np.dot(grad_out,np.transpose(self.W))\n",
    "        \n",
    "        self.W -= self.lr * dW\n",
    "        self.b -= self.lr * db\n",
    "        \n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def crossentropy_loss(y_pred, t_train):\n",
    "    loss = cross_entropy_error(y_pred, t_train)\n",
    "    return loss\n",
    "\n",
    "def grad_crossentropy_loss(y_pred, t_train):\n",
    "    batch_size=t_train.shape[0]\n",
    "    dx=(y_pred-t_train)/batch_size\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1053604045467214"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array([[0.1,0.9,0],[0,0.1,0.9]])\n",
    "t=np.array([[0,1,0],[0,0,1]])\n",
    "crossentropy_loss(y, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    #print(type(len(logits)))\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_mnist\n",
    "(x_train, t_train),(x_test,t_test)= load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "mnist_data=np.array([x_train,t_train,x_test,t_test])\n",
    "mnist_data[1]\n",
    "print(type(mnist_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mnist_data)):\n",
    "    print(mnist_data[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mnist_data)):\n",
    "    print(mnist_data[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# input_size=6\n",
    "# hidden_size=[8,8]\n",
    "# output_size=1\n",
    "# learning_rate=0.01\n",
    "\n",
    "\n",
    "# network=[]\n",
    "# network.append(Hidden(input_size, hidden_size[0], learning_rate=self.lr))\n",
    "# network.append(Hidden(hidden_size[1], hidden_size[2], learning_rate=self.lr))\n",
    "\n",
    "class FNN:\n",
    "    def __init__(self, input_size=6, hidden_size=[8,8], output_size=1,\n",
    "                 learning_rate=0.01\n",
    "                 ): # list with all train/test data\n",
    "        self.lr=learning_rate\n",
    "        \n",
    "#         self.X_train=data_input[0]\n",
    "#         self.t_train=data_input[1]\n",
    "#         self.X_test=data_input[2]\n",
    "#         self.t_test=data_input[3]\n",
    "        \n",
    "        self.network=[]\n",
    "        self.network.append(Hidden(input_size, hidden_size[0], learning_rate=self.lr))\n",
    "        self.network.append(Hidden(hidden_size[0], hidden_size[1], learning_rate=self.lr))\n",
    "        self.network.append(Hidden(hidden_size[1], output_size, learning_rate=self.lr))\n",
    "        self.network.append(tanh())\n",
    "        #self.network.append(RELU())\n",
    "        \n",
    "    def predict(self, x_train): \n",
    "        each_output = []\n",
    "        X=x_train\n",
    "    \n",
    "        for i in range(len(self.network)):\n",
    "            each_output.append(self.network[i].forward(X))\n",
    "            X = self.network[i].forward(X)\n",
    "\n",
    "        return each_output\n",
    "    \n",
    "    def train(self, X_train, t_train):\n",
    "        activations = self.predict(X_train)\n",
    "        for i in range(len(activations)):\n",
    "            print(activations[i].shape)\n",
    "            \n",
    "        pred_y = activations[-1]\n",
    "        print(\"y:\"+str(pred_y.shape))\n",
    "        print(pred_y[0])\n",
    "        print(\"t:\"+str(t_train.shape))\n",
    "        print(t_train[0])\n",
    "        \n",
    "        loss = crossentropy_loss(pred_y, t_train)\n",
    "        print(\"loss\"+str(loss))\n",
    "        loss_grad = grad_crossentropy_loss(pred_y, t_train)\n",
    "        print(\"loss_grad\"+str(loss_grad.shape)+\"\\n\")\n",
    "        #print(loss_grad)\n",
    "        \n",
    "        for i in range(1,len(self.network)):\n",
    "            print(len(self.network) - i)\n",
    "            print(\"network:\"+str(self.network[len(self.network) - i]))\n",
    "            loss_grad = self.network[len(self.network) - i].backward(activations[len(self.network) - i -1], loss_grad)\n",
    "            print(\"x:\"+str(activations[len(self.network) - i -1].shape))\n",
    "            print(\"loss_grad:\"+str(loss_grad.shape))\n",
    "    \n",
    "            \n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def acuuracy(self, X_test, t_test):\n",
    "        results = self.forward(self.X_test)\n",
    "        pred_ = results[-1]\n",
    "        \n",
    "        \n",
    "        y = np.argmax(pred_, axis=1)\n",
    "        if t_test.ndim!=1: \n",
    "            t_test=np.argmax(t_test, axis=1)\n",
    "        score = np.sum(t_test==y)/float(y.shape[0])\n",
    "        \n",
    "        return score\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 13)\n",
      "(100, 8)\n",
      "(100, 10)\n",
      "(100, 10)\n",
      "y:(100, 10)\n",
      "[-1.93418277e-04  5.06786840e-05  7.38489238e-05  1.18017983e-04\n",
      "  7.93939410e-05  1.04171472e-04  7.97418071e-05 -6.76114906e-05\n",
      "  5.42271335e-05 -2.71349120e-05]\n",
      "t:(100, 10)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "lossnan\n",
      "loss_grad(100, 10)\n",
      "\n",
      "3\n",
      "network:<__main__.tanh object at 0x000001CB6F2A7E88>\n",
      "x:(100, 10)\n",
      "loss_grad:(10, 10)\n",
      "2\n",
      "network:<__main__.Hidden object at 0x000001CB6F2A71C8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yexue\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in log\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (8,100) and (10,10) not aligned: 100 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-261-55f8bfef24b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mt_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-260-29bc2835cf8f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, t_train)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"network:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mloss_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss_grad:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-256-c317c1c6e8d0>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, x, grad_out)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# x=input of data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;31m#        dx = np.dot(np.transpose(grad_out), self.W)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (8,100) and (10,10) not aligned: 100 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss_list=[]\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "\n",
    "\n",
    "iters_num=10000\n",
    "batch_size=100\n",
    "lr=0.1\n",
    "train_size=x_train.shape[0]\n",
    "\n",
    "network = FNN(input_size=784, hidden_size=[13,8], output_size=10,\n",
    "                 learning_rate=lr)\n",
    "\n",
    "for i in range(iters_num): #进行iters_num次计算\n",
    "    #获取mini_batch\n",
    "    batch_mask=np.random.choice(train_size,batch_size)\n",
    "    x_batch=x_train[batch_mask]\n",
    "    t_batch=t_train[batch_mask]\n",
    "        \n",
    "    loss=network.train(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    #计算每个epoch的精度\n",
    "    if i%iter_per_epoch ==0:\n",
    "        train_acc=network.accuracy(x_train,t_train)\n",
    "        test_acc=network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \"+str(train_acc)+\",\"+str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False]\n",
      " [ True  True  True]]\n",
      "[[-1.98236587 -1.43953054 -1.15220995]\n",
      " [ 1.29089336  0.17021564  1.10872716]]\n",
      "[[-0.9627603  -0.89360318 -0.81848486]\n",
      " [ 0.85936033  0.16859056  0.803612  ]]\n"
     ]
    }
   ],
   "source": [
    "a=np.random.randn(2,3)\n",
    "print(a>0)\n",
    "print(a)\n",
    "print(tanh().forward(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
